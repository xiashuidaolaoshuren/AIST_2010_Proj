{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Hand Landmarks\n",
    "`data_dir` should contain subdirectories for each gesture, while each subdirectory should contain video(s) / photo(s) of that gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def extract_landmarks(image):\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.7) as hands:\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = []\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                for lm in hand_landmarks.landmark:\n",
    "                    landmarks.append(lm.x)\n",
    "                    landmarks.append(lm.y)\n",
    "            # If only one hand is detected, pad with zeros\n",
    "            if len(results.multi_hand_landmarks) == 1:\n",
    "                landmarks.extend([0] * (21 * 2))  # 21 landmarks, each with x and y\n",
    "            return np.array(landmarks).flatten()\n",
    "    # If no hands are detected, return a zero array\n",
    "    return np.zeros(21 * 2 * 2)  # 21 landmarks, each with x and y, for 2 hands\n",
    "\n",
    "# Example: Extract landmarks from frames in videos\n",
    "# data_dir should contain subdirectories for each gesture, while each subdirectory should contain videos / photos of that gesture\n",
    "data_dir = \"/Users/oscarzhang/Desktop/handgestures_dataset\"\n",
    "landmarks_list = []\n",
    "labels_list = []\n",
    "\n",
    "for label in sorted(os.listdir(data_dir)):\n",
    "    label_dir = os.path.join(data_dir, label)\n",
    "    if os.path.isdir(label_dir):\n",
    "        print(f\"Processing gesture: {label}\")\n",
    "        video_count = 0 # For image: images_count = 0\n",
    "        for video_file in os.listdir(label_dir): # For image: for image_file in os.listdir(label_dir):\n",
    "            video_path = os.path.join(label_dir, video_file) # For image: image_path = os.path.join(label_dir, image_file)\n",
    "            cap = cv2.VideoCapture(video_path) # For image: image = cv2.imread(image_path)\n",
    "            frame_count = 0 # For image: image_count = 0\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret: # For image: if image is None:\n",
    "                    break\n",
    "                frame_count += 1\n",
    "                # If you want to speed up the training, skip frames for not redundancy\n",
    "                # e.g. if frame_count % 5 == 0:, and then you can skip 4 frames, remember to indent the following 4 lines of code\n",
    "                landmarks = extract_landmarks(frame)\n",
    "                if landmarks is not None:\n",
    "                    landmarks_list.append(landmarks)\n",
    "                    labels_list.append(label)\n",
    "            cap.release()\n",
    "            video_count += 1\n",
    "        print(f\"Processed {video_count} videos for gesture: {label}\")\n",
    "\n",
    "X = np.array(landmarks_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "print(f\"Total frames processed: {len(X)}\")\n",
    "print(f\"Total labels: {len(y)}\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_file_path = \"gesture_model.pkl\"\n",
    "joblib.dump(model, model_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
